'use strict';

var config = require('hardhat/config');
var taskNames = require('hardhat/builtin-tasks/task-names');
var ioDevtools = require('@layerzerolabs/io-devtools');
var swag = require('@layerzerolabs/io-devtools/swag');
var devtools = require('@layerzerolabs/devtools');
var pMemoize = require('p-memoize');
var artifacts = require('hardhat/internal/artifacts');
var contracts = require('@ethersproject/contracts');
var devtoolsEvm = require('@layerzerolabs/devtools-evm');
var context = require('hardhat/internal/context');
require('hardhat/internal/core/config/config-loading');
var runtimeEnvironment = require('hardhat/internal/core/runtime-environment');
require('@nomiclabs/hardhat-ethers/internal/ethers-provider-wrapper');
var memoize = require('micro-memoize');
var errors = require('hardhat/internal/core/errors');
var errorsList = require('hardhat/internal/core/errors-list');
var lzDefinitions = require('@layerzerolabs/lz-definitions');
var assert = require('assert');
require('hardhat-deploy/dist/src/type-extensions');
require('hardhat/types/config');
var path = require('path');
var _function = require('fp-ts/lib/function');
var R = require('fp-ts/Record');
var child_process = require('child_process');
var fs = require('fs');
var RR = require('fp-ts/ReadonlyRecord');
var exportDeployments = require('@layerzerolabs/export-deployments');
var providers = require('@ethersproject/providers');

function _interopDefault (e) { return e && e.__esModule ? e : { default: e }; }

function _interopNamespace(e) {
  if (e && e.__esModule) return e;
  var n = Object.create(null);
  if (e) {
    Object.keys(e).forEach(function (k) {
      if (k !== 'default') {
        var d = Object.getOwnPropertyDescriptor(e, k);
        Object.defineProperty(n, k, d.get ? d : {
          enumerable: true,
          get: function () { return e[k]; }
        });
      }
    });
  }
  n.default = e;
  return Object.freeze(n);
}

var pMemoize__default = /*#__PURE__*/_interopDefault(pMemoize);
var memoize__default = /*#__PURE__*/_interopDefault(memoize);
var assert__default = /*#__PURE__*/_interopDefault(assert);
var R__namespace = /*#__PURE__*/_interopNamespace(R);
var RR__namespace = /*#__PURE__*/_interopNamespace(RR);

// src/tasks/deploy.ts

// src/constants/tasks.ts
var TASK_LZ_DEPLOY = "lz:deploy";
var TASK_LZ_EXPORT_DEPLOYMENTS_TYPESCRIPT = "lz:export:deployments:typescript";
var SUBTASK_LZ_SIGN_AND_SEND = "::lz:sign-and-send";
var TASK_LZ_TEST_SIMULATION_START = "lz:test:simulation:start";
var TASK_LZ_TEST_SIMULATION_LOGS = "lz:test:simulation:logs";
var TASK_LZ_TEST_SIMULATION_STOP = "lz:test:simulation:stop";
var TASK_LZ_VALIDATE_SAFE_CONFIGS = "lz:healthcheck:validate:safe-configs";
var TASK_LZ_VALIDATE_RPCS = "lz:healthcheck:validate:rpcs";

// src/errors/errors.ts
var ConfigurationError = class extends Error {
};
var getAllArtifacts = pMemoize__default.default(async (hre = getDefaultRuntimeEnvironment()) => {
  var _a, _b;
  const externalContracts = (_b = (_a = hre.config.external) == null ? void 0 : _a.contracts) != null ? _b : [];
  const artifactsPaths = [
    hre.config.paths.artifacts,
    hre.config.paths.imports,
    ...externalContracts.flatMap(({ artifacts }) => artifacts)
  ];
  const artifactsObjects = artifactsPaths.map((path) => new artifacts.Artifacts(path));
  const artifactses = await Promise.all(artifactsObjects.map(getAllArtifactsFrom));
  return artifactses.flat();
});
var getAllArtifactsFrom = async (artifactsObject) => {
  const fullyQualifiedNames = await artifactsObject.getAllFullyQualifiedNames();
  return fullyQualifiedNames.map((name) => artifactsObject.readArtifactSync(name));
};
var isErrorFragment = (fragment) => fragment.type === "error";
pMemoize__default.default(async () => {
  const artifacts = await getAllArtifacts();
  const abi = artifacts.flatMap((artifact) => artifact.abi).filter(isErrorFragment);
  const deduplicatedAbi = Object.values(Object.fromEntries(abi.map((abi2) => [JSON.stringify(abi2), abi2])));
  return { eid: -1, contract: new contracts.Contract(devtoolsEvm.makeZeroAddress(), deduplicatedAbi) };
});
var getDefaultContext = () => {
  try {
    return context.HardhatContext.getHardhatContext();
  } catch (error) {
    throw new ConfigurationError(`Could not get Hardhat context: ${error}`);
  }
};
var getDefaultRuntimeEnvironment = () => {
  const context = getDefaultContext();
  try {
    return context.getHardhatRuntimeEnvironment();
  } catch (error) {
    throw new ConfigurationError(`Could not get Hardhat Runtime Environment: ${error}`);
  }
};
var getHreByNetworkName = pMemoize__default.default(async (networkName) => {
  const context = getDefaultContext();
  const environment2 = getDefaultRuntimeEnvironment();
  try {
    return new runtimeEnvironment.Environment(
      environment2.config,
      {
        ...environment2.hardhatArguments,
        network: networkName
      },
      environment2.tasks,
      environment2.scopes,
      context.environmentExtenders,
      environment2.userConfig,
      context.providerExtenders
      // This is a bit annoying - the environmentExtenders are not stronly typed
      // so TypeScript complains that the properties required by HardhatRuntimeEnvironment
      // are not present on HardhatRuntimeEnvironmentImplementation
    );
  } catch (error) {
    throw new ConfigurationError(`Could not setup Hardhat Runtime Environment: ${error}`);
  }
});
var getEidsByNetworkName = memoize__default.default(
  (hre = getDefaultRuntimeEnvironment()) => {
    const networkEntries = Object.entries(hre.config.networks);
    const eidEntries = networkEntries.map(
      ([networkName, networkConfig]) => [networkName, networkConfig.eid]
    );
    const eidsByNetworkName = Object.fromEntries(eidEntries);
    const eidEntriesWithDefinedEid = eidEntries.filter(([_, eid2]) => eid2 != null);
    const definedEidsByNetworkName = Object.fromEntries(eidEntriesWithDefinedEid);
    const allDefinedEids = new Set(Object.values(definedEidsByNetworkName));
    const allNetworkNames = new Set(Object.keys(definedEidsByNetworkName));
    if (allDefinedEids.size === allNetworkNames.size) {
      return eidsByNetworkName;
    }
    const duplicatedNetworkNames = Array.from(allDefinedEids).map(
      (eid2) => eidEntriesWithDefinedEid.flatMap(
        ([networkName, definedEid]) => eid2 === definedEid ? [networkName] : []
      )
    ).filter((networkNames) => networkNames.length > 1);
    const messages = duplicatedNetworkNames.map(
      (networkNames) => `- ${networkNames.join(", ")} have eid set to ${devtools.formatEid(eidsByNetworkName[networkNames[0]])}`
    ).join("\n");
    throw new Error(
      `Found multiple networks configured with the same 'eid':

${messages}

Please fix this in your hardhat config.`
    );
  }
);
var csv = {
  name: "csv",
  parse(name, value) {
    return devtools.splitCommaSeparated(value);
  },
  validate() {
  }
};
var isEnvironment = (value) => Object.values(lzDefinitions.Environment).includes(value);
var environment = {
  name: "environment",
  parse(name, value) {
    if (!isEnvironment(value)) {
      throw new errors.HardhatError(errorsList.ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name,
        type: "environment"
      });
    }
    return value;
  },
  validate() {
  }
};
var isStage = (value) => Object.values(lzDefinitions.Stage).includes(value);
var stage = {
  name: "stage",
  parse(name, value) {
    if (!isStage(value)) {
      throw new errors.HardhatError(errorsList.ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name,
        type: "stage"
      });
    }
    return value;
  },
  validate() {
  }
};
var eid = {
  name: "eid",
  parse(name, value) {
    const valueAsInt = parseInt(value);
    if (isNaN(valueAsInt)) {
      const eid2 = lzDefinitions.EndpointId[value];
      if (typeof eid2 !== "number") {
        throw new errors.HardhatError(errorsList.ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
          value,
          name,
          type: "stage"
        });
      }
      return eid2;
    }
    const eidLabel = lzDefinitions.EndpointId[valueAsInt];
    if (typeof eidLabel !== "string") {
      throw new errors.HardhatError(errorsList.ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name,
        type: "stage"
      });
    }
    return valueAsInt;
  },
  validate() {
  }
};
var logLevel = {
  name: "logLevel",
  parse(name, value) {
    if (!ioDevtools.isLogLevel(value)) {
      throw new errors.HardhatError(errorsList.ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name,
        type: "logLevel"
      });
    }
    return value;
  },
  validate() {
  }
};
var fn = {
  name: "function",
  parse: (argName, value) => {
    if (typeof value !== "function") {
      throw new errors.HardhatError(errorsList.ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name: argName,
        type: fn.name
      });
    }
    return value;
  },
  validate() {
  }
};
var signer = {
  name: "signer",
  parse: (argName, value) => {
    if (devtoolsEvm.isEVMAddress(value)) {
      return { type: "address", address: value };
    }
    const parsed = parseInt(value, 10);
    if (!isNaN(parsed)) {
      if (parsed < 0) {
        throw new errors.HardhatError(errorsList.ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
          value,
          name: argName,
          type: signer.name
        });
      }
      return { type: "index", index: parsed };
    }
    return { type: "named", name: value };
  },
  validate() {
  }
};
var types = { csv, eid, logLevel, fn, signer, environment, stage, ...config.types };
function assertHardhatDeploy(hre) {
  assert__default.default(hre.deployments, `You don't seem to be using hardhat-deploy in your project`);
}
function assertDefinedNetworks(networkNames, hre = getDefaultRuntimeEnvironment()) {
  const definedNetworkNames = new Set(Object.keys(getEidsByNetworkName(hre)));
  for (const networkName of networkNames) {
    if (definedNetworkNames.has(networkName)) {
      continue;
    }
    throw new assert.AssertionError({
      message: `Network '${networkName}' has not been defined. Defined networks are ${Array.from(definedNetworkNames).join(", ")}`
    });
  }
  return networkNames;
}
var action = async ({ networks: networksArgument, tags: tagsArgument = [], logLevel: logLevel2 = "info", ci = false, reset = false, stage: stage2 }, hre) => {
  swag.printLogo();
  assertDefinedNetworks(networksArgument != null ? networksArgument : []);
  ioDevtools.setDefaultLogLevel(logLevel2);
  const logger3 = ioDevtools.createLogger();
  const isInteractive = !ci;
  logger3.debug(isInteractive ? "Running in interactive mode" : "Running in non-interactive (CI) mode");
  logger3.debug(reset ? "Will delete existing deployments" : "Will not delete existing deployments");
  try {
    logger3.info(`Compiling your hardhat project`);
    await hre.run(taskNames.TASK_COMPILE);
  } catch (error) {
    logger3.warn(`Failed to compile the project: ${error}`);
  }
  if (networksArgument != null && stage2 != null) {
    logger3.error(`--stage ${stage2} cannot be used in conjunction with --networks ${networksArgument.join(",")}`);
    process.exit(1);
  }
  const eidsByNetworks = Object.entries(getEidsByNetworkName());
  const filteredEidsByNetworks = stage2 == null ? eidsByNetworks : eidsByNetworks.filter(([, eid2]) => eid2 != null && lzDefinitions.endpointIdToStage(eid2) === stage2);
  const configuredNetworkNames = filteredEidsByNetworks.flatMap(([name, eid2]) => eid2 == null ? [] : [name]);
  const networks = networksArgument != null ? networksArgument : configuredNetworkNames;
  let selectedNetworks;
  let selectedTags;
  if (isInteractive) {
    const networksSet = new Set(networks);
    const options = eidsByNetworks.map(([networkName, eid2]) => ({
      title: networkName,
      value: networkName,
      disabled: eid2 == null,
      selected: networksSet.has(networkName),
      hint: eid2 == null ? void 0 : `Connected to ${devtools.formatEid(eid2)}`
    })).sort(
      (a, b) => (
        // We want to show the enabled networks first
        Number(a.disabled) - Number(b.disabled) || //  And sort the rest by their name
        a.title.localeCompare(b.title)
      )
    );
    selectedNetworks = await ioDevtools.promptToSelectMultiple("Which networks would you like to deploy?", { options });
    selectedTags = await ioDevtools.promptForText("Which deploy script tags would you like to use?", {
      defaultValue: tagsArgument == null ? void 0 : tagsArgument.join(","),
      hint: "Leave empty to use all deploy scripts"
    }).then(devtools.splitCommaSeparated);
  } else {
    selectedNetworks = networks;
    selectedTags = tagsArgument;
  }
  if (selectedNetworks.length === 0) {
    return logger3.warn(`No networks selected, exiting`), {};
  }
  logger3.info(
    ioDevtools.pluralizeNoun(
      selectedNetworks.length,
      `Will deploy 1 network: ${selectedNetworks.join(",")}`,
      `Will deploy ${selectedNetworks.length} networks: ${selectedNetworks.join(", ")}`
    )
  );
  if (selectedTags.length === 0) {
    logger3.warn(`Will use all deployment scripts`);
  } else {
    logger3.info(`Will use deploy scripts tagged with ${selectedTags.join(", ")}`);
  }
  const shouldDeploy = isInteractive ? await ioDevtools.promptToContinue() : true;
  if (!shouldDeploy) {
    return logger3.verbose(`User cancelled the operation, exiting`), {};
  }
  logger3.verbose(`Running deployment scripts`);
  const progressBar = swag.render(swag.createProgressBar({ before: "Deploying... ", after: ` 0/${selectedNetworks.length}` }));
  let numProcessed = 0;
  const results = {};
  await Promise.all(
    selectedNetworks.map(async (networkName) => {
      const env = await getHreByNetworkName(networkName);
      try {
        assertHardhatDeploy(env);
        const deploymentsBefore = await env.deployments.all();
        const deploymentsAfter = await env.deployments.run(selectedTags, {
          // If we don't pass resetmemory or set it to true,
          // hardhat deploy will erase the database of deployments
          // (including the external deployments)
          //
          // In effect this means the deployments for LayerZero artifacts would not be available
          resetMemory: false,
          writeDeploymentsToFiles: true,
          deletePreviousDeployments: reset
        });
        const contracts = Object.fromEntries(
          Object.entries(deploymentsAfter).filter(
            ([name]) => !devtools.isDeepEqual(deploymentsBefore[name], deploymentsAfter[name])
          )
        );
        results[networkName] = { contracts };
        logger3.debug(`Successfully deployed network ${networkName}`);
      } catch (error) {
        results[networkName] = { error };
        logger3.debug(`Failed deploying network ${networkName}: ${error}`);
      } finally {
        numProcessed++;
        progressBar.rerender(
          swag.createProgressBar({
            before: "Deploying... ",
            after: ` ${numProcessed}/${selectedNetworks.length}`,
            progress: numProcessed / selectedNetworks.length
          })
        );
      }
    })
  );
  progressBar.clear();
  const errors = Object.entries(results).flatMap(
    ([networkName, { error }]) => error == null ? [] : [{ networkName, error }]
  );
  if (errors.length === 0) {
    return logger3.info(`${ioDevtools.printBoolean(true)} Your contracts are now deployed`), results;
  }
  logger3.error(
    `${ioDevtools.printBoolean(false)} ${ioDevtools.pluralizeNoun(errors.length, "Failed to deploy 1 network", `Failed to deploy ${errors.length} networks`)}`
  );
  const previewErrors = isInteractive ? await ioDevtools.promptToContinue(`Would you like to see the deployment errors?`) : true;
  if (previewErrors) {
    swag.printRecords(
      errors.map(({ networkName, error }) => ({
        Network: networkName,
        Error: String(error)
      }))
    );
  }
  process.exitCode = process.exitCode || 1;
  return results;
};
config.task(TASK_LZ_DEPLOY, "Deploy LayerZero contracts", action).addParam(
  "networks",
  "List of comma-separated networks. If not provided, all networks will be deployed",
  void 0,
  types.csv,
  true
).addParam(
  "tags",
  "List of comma-separated deploy script tags to deploy. If not provided, all deploy scripts will be executed",
  void 0,
  types.csv,
  true
).addParam("logLevel", "Logging level. One of: error, warn, info, verbose, debug, silly", "info", types.logLevel).addParam("stage", "Chain stage. One of: mainnet, testnet, sandbox", void 0, types.stage, true).addFlag("ci", "Continuous integration (non-interactive) mode. Will not ask for any input from the user").addFlag("reset", "Delete existing deployments");
var resolveSimulationConfig = (userConfig, hardhatConfig) => {
  var _a, _b, _c;
  return {
    port: (_a = userConfig.port) != null ? _a : 8545,
    directory: path.resolve(hardhatConfig.paths.root, (_b = userConfig.directory) != null ? _b : ".layerzero"),
    overwriteAccounts: (_c = userConfig.overwriteAccounts) != null ? _c : true,
    anvil: {
      // For now we'll hardcode the mnemonic we'll use to seed the accounts on the simulation networks
      mnemonic: "test test test test test test test test test test test junk",
      ...userConfig.anvil,
      // The host and port need to always point to 0.0.0.0:8545
      // since anvil runs in the container that exposes this port on 0.0.0.0
      host: "0.0.0.0",
      port: 8545
    }
  };
};
var getAnvilOptionsFromHardhatNetworks = (config, networksConfig) => _function.pipe(
  networksConfig,
  // We want to drop all the networks that don't have URLs
  R__namespace.filter(isHttpNetworkConfig),
  // And map the network configs into AnvilOptions
  R__namespace.map(
    (networkConfig) => ({
      ...config.anvil,
      forkUrl: networkConfig.url
    })
  )
);
var pickNetworkConfigs = (networks) => R__namespace.filterWithIndex((networkName) => networks.includes(networkName));
var isHttpNetworkConfig = (networkConfig) => "url" in networkConfig && typeof networkConfig.url === "string";
var action2 = async ({ logLevel: logLevel2 = "info", follow = false }, hre) => {
  var _a, _b, _c;
  ioDevtools.setDefaultLogLevel(logLevel2);
  swag.printLogo();
  const logger3 = ioDevtools.createLogger();
  const simulationUserConfig = (_c = (_b = (_a = hre.userConfig.layerZero) == null ? void 0 : _a.experimental) == null ? void 0 : _b.simulation) != null ? _c : {};
  logger3.verbose(`Using simulation user config:
${ioDevtools.printJson(simulationUserConfig)}`);
  const simulationConfig = resolveSimulationConfig(simulationUserConfig, hre.config);
  logger3.verbose(`Resolved simulation config:
${ioDevtools.printJson(simulationConfig)}`);
  const dockerComposePath = path.join(simulationConfig.directory, "docker-compose.yaml");
  if (!ioDevtools.isFile(dockerComposePath)) {
    logger3.warn(`Could not find simulation docker compose file '${dockerComposePath}'`);
    logger3.warn(`Did you run 'npx hardhat ${TASK_LZ_TEST_SIMULATION_START}'?`);
    process.exitCode = 1;
    return;
  }
  try {
    logger3.verbose(`Spawning docker compose logs command for ${dockerComposePath}`);
    const command = ["compose", "-f", dockerComposePath, "logs", ...follow ? ["--follow"] : []];
    child_process.spawnSync("docker", command, { stdio: "inherit" });
  } catch (error) {
    logger3.error(`Failed to spawn docker compose logs command for ${dockerComposePath}: ${error}`);
    process.exitCode = 1;
    return;
  }
};
if (process.env.LZ_ENABLE_EXPERIMENTAL_SIMULATION) {
  config.task(TASK_LZ_TEST_SIMULATION_LOGS, "Show logs for LayerZero omnichain simulation", action2).addParam("logLevel", "Logging level. One of: error, warn, info, verbose, debug, silly", "info", types.logLevel).addFlag("follow", "Follow log output");
}
var createEvmNodeServiceSpec = (anvilOptions) => ({
  // This service references a Dockerfile that is copied
  // next to the resulting docker-compose.yaml
  //
  // The source for this Dockerfile is located in src/simulation/assets/Dockerfile.conf
  build: {
    dockerfile: "Dockerfile",
    target: "node-evm"
  },
  command: ["anvil", ...devtoolsEvm.createAnvilCliOptions(anvilOptions)]
});
var createEvmNodeProxyServiceSpec = (port, networkServices) => ({
  // This service references a Dockerfile that is copied
  // next to the resulting docker-compose.yaml
  //
  // The source for this Dockerfile is located in src/simulation/assets/Dockerfile.conf
  build: {
    dockerfile: "Dockerfile",
    target: "proxy-evm"
  },
  // This service will expose its internal 8545 port to a host port
  //
  // The internal 8545 port is hardcoded both here and in the nginx.conf file,
  // the source for which is located in src/simulation/assets/nginx.conf
  ports: [`${port}:8545`],
  depends_on: _function.pipe(
    networkServices,
    // This service will depend on the RPCs to be healthy
    // so we'll take the networkServices object and replace
    // the values with service_healthy condition
    RR__namespace.map(() => ({
      condition: "service_healthy"
    }))
  )
});
var createSimulationComposeSpec = (config, networks) => ({
  services: _function.pipe(
    networks,
    // First we turn the networks into docker compose specs for EVM nodes
    RR__namespace.map(createEvmNodeServiceSpec),
    (networkServiceSpecs) => (
      // Then we add the RPC proxy server
      //
      // There is a small edge case here that we can address
      // if it ever comes up: if a network is called 'rpc', this compose file
      // will not work.
      //
      // The fix for this is to prefix all networks with something like network-xxx
      // but we can do that if ever this usecase comes up
      _function.pipe(
        networkServiceSpecs,
        RR__namespace.upsertAt("rpc", createEvmNodeProxyServiceSpec(config.port, networkServiceSpecs))
      )
    )
  )
});

// src/simulation/assets/Dockerfile.conf
var Dockerfile_default = "ARG FOUNDRY_VERSION=nightly-156cb1396b7076c6f9cb56f3719f8c90f7f52064\nARG ALPINE_VERSION=3.18\n\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\n#\n#             Image that gives us the foundry tools\n#\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\nFROM ghcr.io/foundry-rs/foundry:$FOUNDRY_VERSION AS foundry\n\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\n#\n#               Image that starts an EVM node\n#\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\nFROM alpine:$ALPINE_VERSION AS node-evm\n\nSTOPSIGNAL SIGINT\n\n# We will provide a default healthcheck (that assumes that the netowrk is running on the default port 8545)\nHEALTHCHECK --timeout=2s --interval=2s --retries=20 CMD cast block --rpc-url http://localhost:8545/ latest\n\n# Get anvil\nCOPY --from=foundry /usr/local/bin/anvil /usr/local/bin/anvil\n\n# Get cast for healthcheck\nCOPY --from=foundry /usr/local/bin/cast /usr/local/bin/cast\n\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\n#\n#           Image that starts an nginx proxy server\n#\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\nFROM nginx:alpine$ALPINE_VERSION AS proxy-evm\n\nCOPY ./nginx.conf /etc/nginx/nginx.conf\n\nHEALTHCHECK --timeout=2s --interval=2s --retries=20 CMD curl -f http://0.0.0.0:8545/health-check\n\n";

// src/simulation/assets/nginx.conf
var nginx_default = `events {}

http {
  # We will modify the log  format to include the target_network
  log_format proxied '$remote_addr - $remote_user [$time_local] '
                     '"$request" $status $body_bytes_sent '
                     '"$http_referer" "$http_user_agent" '
                     'Network: "$target_network"';

  server {
    # This proxy server will listen on port 8545
    # 
    # Even though it's not ideal to have this hardcoded, this port
    # will be remapped to a desired host port using docker compose,
    # the only issue this hardcoding brings is the fact that this port
    # needs to match the container port in the compose spec
    listen 8545;
    listen [::]:8545;

    # We will add a simple endpoint for healthcheck
    location /health-check {
      access_log	off;
      error_log	off;
      return 200 'ok';
    }

    # In this section we'll proxy all the requests to this server
    # to the respective network nodes
    # 
    # The requests are proxied based on the first path segment:
    # 
    # http://localhost/fuji -> http://fuji:8545/
    # 
    # For now the remaining path segments are not being preserved:
    # 
    # # http://localhost/fuji/some/url -> http://fuji:8545/
    location / {
      # Set the log format to be our custom 'proxied' log format
      access_log /var/log/nginx/access.log proxied;

      resolver 127.0.0.11;
      autoindex off;

      # This variable will hold the name of the network to proxy to
      set $target_network '';

      # Extract the first path segment from the request URI
      if ($request_uri ~* ^/(?<target_network>[^/]+)(/.*)?$) {
        set $target_network $1;
      }

      # Proxy the request to the appropriate network
      proxy_pass http://$target_network:8545/;
    }
  }
}`;
var action3 = async ({ networks: networksArgument, daemon = false, logLevel: logLevel2 = "info", stage: stage2 }, hre) => {
  var _a, _b, _c;
  ioDevtools.setDefaultLogLevel(logLevel2);
  swag.printLogo();
  const logger3 = ioDevtools.createLogger();
  if (networksArgument != null && stage2 != null) {
    logger3.error(`--stage ${stage2} cannot be used in conjunction with --networks ${networksArgument.join(",")}`);
    process.exit(1);
  }
  const isOnStage = stage2 == null ? () => true : (eid2) => lzDefinitions.endpointIdToStage(eid2) === stage2;
  const networks = networksArgument ? (
    // Here we need to check whether the networks have been defined in hardhat config
    assertDefinedNetworks(networksArgument)
  ) : (
    //  But here we are taking them from hardhat config so no assertion is necessary
    Object.entries(getEidsByNetworkName()).flatMap(
      ([networkName, eid2]) => eid2 != null && isOnStage(eid2) ? [networkName] : []
    )
  );
  if (networks.length === 0) {
    logger3.warn(`No networks with eid configured, exiting`);
    return;
  }
  logger3.info(`Will create a simulation configuration for networks ${networks.join(", ")}`);
  const simulationUserConfig = (_c = (_b = (_a = hre.userConfig.layerZero) == null ? void 0 : _a.experimental) == null ? void 0 : _b.simulation) != null ? _c : {};
  logger3.verbose(`Using simulation user config:
${ioDevtools.printJson(simulationUserConfig)}`);
  const simulationConfig = resolveSimulationConfig(simulationUserConfig, hre.config);
  logger3.verbose(`Resolved simulation config:
${ioDevtools.printJson(simulationConfig)}`);
  const networkConfigs = pickNetworkConfigs(networks)(hre.config.networks);
  const anvilOptions = getAnvilOptionsFromHardhatNetworks(simulationConfig, networkConfigs);
  logger3.verbose(`The anvil config is:
${ioDevtools.printJson(anvilOptions)}`);
  const composeSpec = createSimulationComposeSpec(simulationConfig, anvilOptions);
  const serializedComposeSpec = devtools.serializeDockerComposeSpec(composeSpec);
  logger3.verbose(`Making sure directory ${simulationConfig.directory} exists`);
  fs.mkdirSync(simulationConfig.directory, { recursive: true });
  const dockerfilePath = path.join(simulationConfig.directory, "Dockerfile");
  logger3.debug(`Writing simulation Dockerfile to ${dockerfilePath}`);
  fs.writeFileSync(dockerfilePath, Dockerfile_default);
  const nginxConfPath = path.join(simulationConfig.directory, "nginx.conf");
  logger3.debug(`Writing simulation nginx configuration file to ${nginxConfPath}`);
  fs.writeFileSync(nginxConfPath, nginx_default);
  const dockerComposePath = path.join(simulationConfig.directory, "docker-compose.yaml");
  logger3.debug(`Writing simulation docker compose spec file to ${dockerComposePath}`);
  fs.writeFileSync(dockerComposePath, serializedComposeSpec);
  try {
    if (daemon) {
      logger3.info(`Starting simulation in the background`);
      logger3.info(
        `Use 'LZ_ENABLE_EXPERIMENTAL_SIMULATION=1 npx hardhat ${TASK_LZ_TEST_SIMULATION_LOGS}' to view the network logs`
      );
    } else {
      logger3.info(`Starting simulation`);
    }
    logger3.verbose(`Spawning docker compose up command for ${dockerComposePath}`);
    const additionalUpArgs = daemon ? ["--wait"] : [];
    const result = child_process.spawnSync("docker", ["compose", "-f", dockerComposePath, "up", ...additionalUpArgs], {
      stdio: "inherit"
    });
    if (result.status !== 0) {
      throw new Error(`docker compose up command failed with exit code ${result.status}`);
    }
  } catch (error) {
    logger3.error(`Failed to spawn docker compose up command for ${dockerComposePath}: ${error}`);
    process.exitCode = 1;
    return;
  }
};
if (process.env.LZ_ENABLE_EXPERIMENTAL_SIMULATION) {
  config.task(TASK_LZ_TEST_SIMULATION_START, "Start LayzerZero omnichain simulation", action3).addParam("logLevel", "Logging level. One of: error, warn, info, verbose, debug, silly", "info", types.logLevel).addParam("networks", "Comma-separated list of networks to simulate", void 0, types.csv, true).addParam("stage", "Chain stage. One of: mainnet, testnet, sandbox", void 0, types.stage, true).addFlag("daemon", "Start the simulation in the background");
}
var action4 = async ({ logLevel: logLevel2 = "info" }, hre) => {
  var _a, _b, _c;
  ioDevtools.setDefaultLogLevel(logLevel2);
  swag.printLogo();
  const logger3 = ioDevtools.createLogger();
  const simulationUserConfig = (_c = (_b = (_a = hre.userConfig.layerZero) == null ? void 0 : _a.experimental) == null ? void 0 : _b.simulation) != null ? _c : {};
  logger3.verbose(`Using simulation user config:
${ioDevtools.printJson(simulationUserConfig)}`);
  const simulationConfig = resolveSimulationConfig(simulationUserConfig, hre.config);
  logger3.verbose(`Resolved simulation config:
${ioDevtools.printJson(simulationConfig)}`);
  const dockerComposePath = path.join(simulationConfig.directory, "docker-compose.yaml");
  if (!ioDevtools.isFile(dockerComposePath)) {
    logger3.warn(`Could not find simulation docker compose file '${dockerComposePath}'`);
    logger3.warn(`Did you run 'npx hardhat ${TASK_LZ_TEST_SIMULATION_START}'?`);
    process.exitCode = 1;
    return;
  }
  try {
    logger3.info(`Stopping simulation`);
    logger3.verbose(`Spawning docker compose down command for ${dockerComposePath}`);
    child_process.spawnSync("docker", ["compose", "-f", dockerComposePath, "down"], {
      stdio: "inherit"
    });
  } catch (error) {
    logger3.error(`Failed to spawn docker compose down command for ${dockerComposePath}: ${error}`);
    process.exitCode = 1;
    return;
  } finally {
    try {
      fs.rmSync(simulationConfig.directory, { force: true, recursive: true });
    } catch (error) {
      logger3.error(`Failed to delete '${simulationConfig.directory}': ${error}`);
    }
  }
};
if (process.env.LZ_ENABLE_EXPERIMENTAL_SIMULATION) {
  config.task(TASK_LZ_TEST_SIMULATION_STOP, "Stop LayerZero omnichain simulation", action4).addParam(
    "logLevel",
    "Logging level. One of: error, warn, info, verbose, debug, silly",
    "info",
    types.logLevel
  );
}
config.subtask(
  SUBTASK_LZ_SIGN_AND_SEND,
  "Sign and send a list of transactions using a local signer",
  ({ transactions, ...args }) => devtools.createSignAndSendFlow(args)({ transactions })
).addFlag("ci", "Continuous integration (non-interactive) mode. Will not ask for any input from the user").addParam("transactions", "List of OmniTransaction objects", void 0, types.any).addParam("createSigner", "Function that creates a signer for a particular network", void 0, types.fn).addParam("logger", "Logger object (see @layerzerolabs/io-devtools", void 0, types.any, true).addParam("onFailure", "Function that handles sign & send failures", void 0, types.fn, true);
var action5 = async ({ networks, contracts, logLevel: logLevel2 = "info", outDir = "generated" }, hre) => {
  swag.printLogo();
  ioDevtools.setDefaultLogLevel(logLevel2);
  const logger3 = ioDevtools.createLogger();
  const results = exportDeployments.generate({
    // Since we are in a hardhat project, the deployments path is coming from the config
    deploymentsDir: hre.config.paths.deployments,
    outDir,
    includeDeploymentFile: exportDeployments.createIncludeDirent(contracts),
    includeNetworkDir: exportDeployments.createIncludeDirent(networks),
    generator: exportDeployments.generatorTypeScript
  });
  logger3.info(
    `${ioDevtools.printBoolean(true)} ${ioDevtools.pluralizeNoun(results.length, `Generated 1 file:`, `Generated ${results.length} files`)}`
  );
  for (const { path } of results) {
    logger3.info(`	${path}`);
  }
  return results;
};
config.task(TASK_LZ_EXPORT_DEPLOYMENTS_TYPESCRIPT, "Export deployments as TypeScript files", action5).addParam(
  "networks",
  "List of comma-separated networks. If not provided, all networks will be deployed",
  void 0,
  types.csv,
  true
).addParam(
  "contracts",
  "List of comma-separated contract names. If not provided, all contracts will be exported",
  void 0,
  types.csv,
  true
).addParam("logLevel", "Logging level. One of: error, warn, info, verbose, debug, silly", "info", types.logLevel);
var SAFE_CONFIG_KEY = "safeConfig";
var logger = ioDevtools.createLogger();
var validateSafeConfig = async (config) => {
  if (!config.safeAddress) {
    logger.error(`${ioDevtools.printBoolean(false)} Missing safeAddress`);
    return false;
  }
  if (!config.safeUrl) {
    logger.error(`${ioDevtools.printBoolean(false)} Missing safeUrl`);
    return false;
  }
  const apiUrl = `${config.safeUrl}/api/v1/safes/${config.safeAddress}/`;
  try {
    const response = await fetch(apiUrl);
    if (!response.ok) {
      return false;
    }
    return true;
  } catch (error) {
    if (error instanceof Error) {
      logger.error(`Validation failed: ${error.message}`);
    } else {
      logger.error("An unknown error occurred");
    }
    return false;
  }
};
var action6 = async (_, hre) => {
  var _a;
  swag.printLogo();
  const networkNames = Object.keys(hre.userConfig.networks || {});
  logger.info(`...Validating safe configs for ${networkNames.join(", ")}...`);
  for (const networkName of networkNames) {
    const networkConfig = (_a = hre.userConfig.networks) == null ? void 0 : _a[networkName];
    if (networkConfig && SAFE_CONFIG_KEY in networkConfig) {
      const isValid = await validateSafeConfig(networkConfig.safeConfig);
      if (!isValid) {
        throw new Error(`${ioDevtools.printBoolean(false)} Safe config validation failed for network: ${networkName}`);
      }
    }
  }
  logger.info(`${ioDevtools.printBoolean(true)} All safe configs are valid!`);
};
config.task(TASK_LZ_VALIDATE_SAFE_CONFIGS, "Validate safe configs in hardhat.config.ts", action6);
var RPC_URL_KEY = "url";
var HTTP_URL = "http://";
var HTTPS_URL = "https://";
var WS_URL = "ws://";
var WSS_URL = "wss://";
var TIMEOUT = 1e3;
var logger2 = ioDevtools.createLogger();
var getProvider = async (rpcUrl, networkName) => {
  let provider;
  if (rpcUrl.startsWith(HTTP_URL) || rpcUrl.startsWith(HTTPS_URL)) {
    provider = new providers.JsonRpcProvider(rpcUrl);
  } else if (rpcUrl.startsWith(WS_URL) || rpcUrl.startsWith(WSS_URL)) {
    provider = new providers.WebSocketProvider(rpcUrl);
  } else {
    logger2.error(`Unsupported RPC protocol in network: ${networkName}`);
  }
  return provider;
};
var action7 = async (taskArgs, hre) => {
  var _a;
  swag.printLogo();
  if (taskArgs.networks != null && taskArgs.stage != null) {
    logger2.error(
      `--stage ${taskArgs.stage} cannot be used in conjunction with --networks ${taskArgs.networks.join(",")}`
    );
    process.exit(1);
  }
  const isOnStage = taskArgs.stage == null ? () => true : (eid2) => lzDefinitions.endpointIdToStage(eid2) === taskArgs.stage;
  const networks = taskArgs.networks ? (
    // Here we need to check whether the networks have been defined in hardhat config
    pickNetworkConfigs(assertDefinedNetworks(taskArgs.networks))(hre.config.networks)
  ) : taskArgs.stage ? pickNetworkConfigs(
    Object.entries(getEidsByNetworkName()).flatMap(
      ([networkName, eid2]) => eid2 != null && isOnStage(eid2) ? [networkName] : []
    )
  )(hre.config.networks) : hre.config.networks;
  const eidByNetworkName = getEidsByNetworkName(hre);
  logger2.info(
    `========== Validating RPC URLs for networks: ${((_a = taskArgs.networks) == null ? void 0 : _a.join(", ")) || Object.keys(eidByNetworkName).join(", ")}`
  );
  const networksWithInvalidRPCs = [];
  await Promise.all(
    Object.entries(eidByNetworkName).map(async ([networkName, eid2]) => {
      var _a2;
      if (!eid2) {
        logger2.info(`No eid found for network ${networkName}, skipping`);
        return;
      }
      const rpcUrl = (_a2 = networks[networkName]) == null ? void 0 : _a2[RPC_URL_KEY];
      if (!rpcUrl) {
        logger2.info(`No RPC URL found for network ${networkName}, skipping`);
        return;
      }
      const provider = await getProvider(rpcUrl, networkName);
      if (!provider) {
        networksWithInvalidRPCs.push(networkName);
        logger2.error(`Error fetching provider for network: ${networkName}`);
        return;
      }
      return Promise.race([
        provider.getBlockNumber(),
        new Promise((_, reject) => setTimeout(reject, taskArgs.timeout))
      ]).then(
        (block) => {
          return !!block;
        },
        () => {
          networksWithInvalidRPCs.push(networkName);
        }
      );
    })
  );
  if (networksWithInvalidRPCs.length !== 0) {
    logger2.error(
      `${ioDevtools.printBoolean(false)} ========== RPC URL validation failed for network(s): ${networksWithInvalidRPCs.join(", ")}`
    );
  } else {
    logger2.info(`${ioDevtools.printBoolean(true)} ========== All RPC URLs are valid!`);
  }
};
config.task(
  TASK_LZ_VALIDATE_RPCS,
  "Validate RPC URLs in hardhat.config.ts. RPCs are only considered valid if they use the https or wss protocol and respond within the specified timeout.",
  action7
).addParam(
  "timeout",
  `Maximum amount of time (in milliseconds) that the RPC URLs have to respond. If unspecified, default timeout of ${TIMEOUT}ms will be used.`,
  TIMEOUT,
  config.types.int,
  true
).addParam("networks", "Comma-separated list of networks to simulate", void 0, types.csv, true).addParam("stage", "Chain stage. One of: mainnet, testnet, sandbox", void 0, types.stage, true);
//# sourceMappingURL=out.js.map
//# sourceMappingURL=index.js.map